{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we assume that the possible categories are fixed, so no LM heads\n",
    "\n",
    "- `Extract` the brand name from the context\n",
    "- `predict` the categories from the fixed set of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 23:31:04.347504: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-02 23:31:04.347530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-02 23:31:04.348290: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-02 23:31:04.352337: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-02 23:31:04.874319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 23:31:05.394560: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.395843: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.418076: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.419499: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.419602: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.421557: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.422963: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.424246: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.425437: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.872071: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.873404: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.874632: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 23:31:05.875847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46080 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# create config to use both gpus on wickerman machine:\n",
    "# 1. Nvidia A40 (25GB memory allocation)\n",
    "# 2. Nvidia RTX 3060 (11GB memory allocation)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU') # use Use Nvidia A40 only\n",
    "\n",
    "log_dev_conf_a40 = tf.config.LogicalDeviceConfiguration(\n",
    "    memory_limit=45*1024 # 25 GB allocation for a40 GPU\n",
    ")\n",
    "\n",
    "tf.config.set_logical_device_configuration(\n",
    "    gpus[0], # select GPU_0, i.e., Nvidia A40\n",
    "    [log_dev_conf_a40] # apply 25GB config\n",
    ")\n",
    "\n",
    "# tf.config.set_visible_devices(gpus[1], 'GPU') # use RTX3060 only\n",
    "\n",
    "# log_dev_conf_rtx3060 = tf.config.LogicalDeviceConfiguration(\n",
    "#     memory_limit=12*1024 # 11 GB allocation for rtx3060 GPU\n",
    "# )\n",
    "\n",
    "# tf.config.set_logical_device_configuration(\n",
    "#     gpus[1], # select GPU_1, i.e., Nvidia rtx3060\n",
    "#     [log_dev_conf_rtx3060] # apply 11GB config\n",
    "# )\n",
    "\n",
    "# MultiGPU setup\n",
    "# Create a MirroredStrategy.\n",
    "# strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from pqdm.threads import pqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 23:31:06.598519: I external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1101] failed to allocate 45.00GiB (48318382080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/vedant/miniconda3/envs/tf/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer, TFRobertaModel\n",
    "\n",
    "model_id = 'roberta-base'\n",
    "\n",
    "roberta = TFRobertaModel.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define new special tokens\n",
    "new_special_tokens = {\n",
    "    'additional_special_tokens': ['<category>', '</category_end>', '<brand>', '</brand_end>']\n",
    "}\n",
    "\n",
    "# Add the new special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens(new_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRobertaClassifier(tf.keras.Model):\n",
    "    def __init__(self, roberta_model, num_classes):\n",
    "        super(CustomRobertaClassifier, self).__init__()\n",
    "        self.roberta = roberta_model.roberta  # Use only the Roberta encoder\n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.3)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Extract the last hidden state from the Roberta model\n",
    "        roberta_output = self.roberta(inputs, training=training)[0]  # shape: (batch_size, seq_length, hidden_size)\n",
    "        pooled_output = self.global_pool(roberta_output)  # shape: (batch_size, hidden_size)\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        logits = self.classifier(pooled_output)  # shape: (batch_size, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path) as input_file:\n",
    "        lines = list(map(json.loads, input_file))\n",
    "\n",
    "    return lines\n",
    "\n",
    "def load_data(root_path):\n",
    "    files = os.listdir(root_path)\n",
    "    dataset= {}\n",
    "\n",
    "    for filename in files:\n",
    "        filepath = root_path + f'/{filename}'\n",
    "\n",
    "        dataset[filename] = load_jsonl(filepath)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data('data/L1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['llama3.1', 'gemma2:2b', 'gemma2']\n",
    "\n",
    "test_from = 'attrebute_val'\n",
    "train_from = 'attrebute_train'\n",
    "target_from = 'attrebute_test'\n",
    "\n",
    "train_inputs = train_from + '.data'\n",
    "train_labels = train_from + '.solution'\n",
    "\n",
    "test_inputs = test_from + '.data'\n",
    "test_labels = test_from + '.solution'\n",
    "\n",
    "target_inputs = target_from + '.data'\n",
    "\n",
    "test_inputs = dataset[test_inputs]\n",
    "test_labels = dataset[test_labels]\n",
    "train_inputs = dataset[train_inputs]\n",
    "train_labels = dataset[train_labels]\n",
    "target_inputs = dataset[target_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = pd.DataFrame.from_records(train_labels)\n",
    "test_labels_df = pd.DataFrame.from_records(test_labels)\n",
    "L0_unique_labels = pd.concat([train_labels_df, test_labels_df])['L0_category'].unique().tolist()\n",
    "L1_unique_labels = pd.concat([train_labels_df, test_labels_df])['L1_category'].unique().tolist()\n",
    "L2_unique_labels = pd.concat([train_labels_df, test_labels_df])['L2_category'].unique().tolist()\n",
    "L3_unique_labels = pd.concat([train_labels_df, test_labels_df])['L3_category'].unique().tolist()\n",
    "L4_unique_labels = pd.concat([train_labels_df, test_labels_df])['L4_category'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(index, k):\n",
    "    one_hot = np.zeros(k)  # Step 1: Initialize an array of zeros\n",
    "    one_hot[index-1] = 1     # Step 2: Set the element at the specified index to 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['L0_category', 'L1_category',\n",
    "       'L2_category', 'L3_category', 'L4_category']\n",
    "\n",
    "train_category_labels = train_labels_df[columns]\n",
    "test_category_labels = test_labels_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_L0 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L0_unique_labels.index(item), len(L0_unique_labels))\n",
    "    for item in train_category_labels['L0_category']], dtype=np.float32\n",
    ")\n",
    "train_category_L1 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L1_unique_labels.index(item), len(L1_unique_labels))\n",
    "    for item in train_category_labels['L1_category']], dtype=np.float32\n",
    ")\n",
    "train_category_L2 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L2_unique_labels.index(item), len(L2_unique_labels))\n",
    "    for item in train_category_labels['L2_category']], dtype=np.float32\n",
    ")\n",
    "train_category_L3 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L3_unique_labels.index(item), len(L3_unique_labels))\n",
    "    for item in train_category_labels['L3_category']], dtype=np.float32\n",
    ")\n",
    "train_category_L4 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L4_unique_labels.index(item), len(L4_unique_labels))\n",
    "    for item in train_category_labels['L4_category']], dtype=np.float32\n",
    ")\n",
    "\n",
    "test_category_L0 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L0_unique_labels.index(item), len(L0_unique_labels))\n",
    "    for item in test_category_labels['L0_category']], dtype=np.float32\n",
    ")\n",
    "test_category_L1 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L1_unique_labels.index(item), len(L1_unique_labels))\n",
    "    for item in test_category_labels['L1_category']], dtype=np.float32\n",
    ")\n",
    "test_category_L2 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L2_unique_labels.index(item), len(L2_unique_labels))\n",
    "    for item in test_category_labels['L2_category']], dtype=np.float32\n",
    ")\n",
    "test_category_L3 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L3_unique_labels.index(item), len(L3_unique_labels))\n",
    "    for item in test_category_labels['L3_category']], dtype=np.float32\n",
    ")\n",
    "test_category_L4 = np.array(\n",
    "    [\n",
    "        one_hot_encode(L4_unique_labels.index(item), len(L4_unique_labels))\n",
    "    for item in test_category_labels['L4_category']], dtype=np.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_length = 25\n",
    "L0_unique_tokens = tokenizer.batch_encode_plus(L0_unique_labels, return_tensors='np', max_length=label_length, padding='max_length').input_ids\n",
    "L1_unique_tokens = tokenizer.batch_encode_plus(L1_unique_labels, return_tensors='np', max_length=label_length, padding='max_length').input_ids\n",
    "L2_unique_tokens = tokenizer.batch_encode_plus(L2_unique_labels, return_tensors='np', max_length=label_length, padding='max_length').input_ids\n",
    "L3_unique_tokens = tokenizer.batch_encode_plus(L3_unique_labels, return_tensors='np', max_length=label_length, padding='max_length').input_ids\n",
    "L4_unique_tokens = tokenizer.batch_encode_plus(L4_unique_labels, return_tensors='np', max_length=label_length, padding='max_length').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 23:31:20.200880: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "L0_unique_embeds = roberta(L0_unique_tokens, return_dict=True)['pooler_output']\n",
    "L1_unique_embeds = roberta(L1_unique_tokens, return_dict=True)['pooler_output']\n",
    "L2_unique_embeds = roberta(L2_unique_tokens, return_dict=True)['pooler_output']\n",
    "L3_unique_embeds = roberta(L3_unique_tokens, return_dict=True)['pooler_output']\n",
    "L4_unique_embeds = roberta(L4_unique_tokens, return_dict=True)['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_embeds(k):\n",
    "    return {\n",
    "        \"L0_embeds\": tf.stack([L0_unique_embeds] * k),\n",
    "        \"L1_embeds\": tf.stack([L1_unique_embeds] * k),\n",
    "        \"L2_embeds\": tf.stack([L2_unique_embeds] * k),\n",
    "        \"L3_embeds\": tf.stack([L3_unique_embeds] * k),\n",
    "        \"L4_embeds\": tf.stack([L4_unique_embeds] * k),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN=100\n",
    "prompt_template=\"Title:{title}\\nStore:{store}\\nManufacturer:{details_Manufacturer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(prompt, l0, l1, l2, l3, l4):\n",
    "    prompt = prompt.numpy().decode('utf-8')\n",
    "    tokens = tokenizer.encode_plus(prompt, max_length=SEQ_LEN, padding='max_length', return_tensors='np')\n",
    "    return (\n",
    "        tokens.input_ids[0][:SEQ_LEN], tokens.attention_mask[0][:SEQ_LEN], \n",
    "        l0, l1, l2, l3, l4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806815287aae47779d8d422e6bcf660f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e9ea2eab6647c19e3c47e4124d5f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237c999538114a1ca7d506b616081819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833106b386b74a3f94768d9920732d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2a29d6c9284d2ca66a4c522263905a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c773459e943a49c78d64a4cdda0bc090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_inputs_prompts = pqdm(train_inputs, lambda a: prompt_template.format(**a), n_jobs=12)\n",
    "test_inputs_prompts = pqdm(test_inputs, lambda a: prompt_template.format(**a), n_jobs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "    (train_inputs_prompts, \n",
    "     train_category_L0, \n",
    "     train_category_L1, \n",
    "     train_category_L2, \n",
    "     train_category_L3, \n",
    "     train_category_L4))\n",
    "    .shuffle(1000)\n",
    "    .map(\n",
    "        lambda prompt, l0, l1, l2, l3, l4: tf.py_function(\n",
    "        preprocess, [prompt, l0, l1, l2, l3, l4], \n",
    "        [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32]), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "    (test_inputs_prompts, \n",
    "     test_category_L0, \n",
    "     test_category_L1, \n",
    "     test_category_L2, \n",
    "     test_category_L3, \n",
    "     test_category_L4))\n",
    "    .shuffle(1000)\n",
    "    .map(\n",
    "        lambda prompt, l0, l1, l2, l3, l4: tf.py_function(\n",
    "        preprocess, [prompt, l0, l1, l2, l3, l4], \n",
    "        [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32]), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (train_inputs_prompts, \n",
    "     train_category_L0, \n",
    "     train_category_L1, \n",
    "     train_category_L2, \n",
    "     train_category_L3, \n",
    "     train_category_L4)\n",
    "del (test_inputs_prompts, \n",
    "     test_category_L0, \n",
    "     test_category_L1, \n",
    "     test_category_L2, \n",
    "     test_category_L3, \n",
    "     test_category_L4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 12\n",
    "key_dim = 64\n",
    "num_cross_attn_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype=tf.int32)\n",
    "attention_mask = layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype=tf.int32)\n",
    "\n",
    "L0_input = layers.Input(shape=(L0_unique_embeds.shape), name='L0_embeds')\n",
    "L1_input = layers.Input(shape=(L1_unique_embeds.shape), name='L1_embeds')\n",
    "L2_input = layers.Input(shape=(L2_unique_embeds.shape), name='L2_embeds')\n",
    "L3_input = layers.Input(shape=(L3_unique_embeds.shape), name='L3_embeds')\n",
    "L4_input = layers.Input(shape=(L4_unique_embeds.shape), name='L4_embeds') \n",
    "\n",
    "prompt_embeds = roberta(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)['last_hidden_state']\n",
    "\n",
    "# L0 predictor\n",
    "# for _ in range(num_cross_attn_blocks):\n",
    "L0_attn_output = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(L0_input, prompt_embeds)\n",
    "L0_embeds = keras.layers.LayerNormalization()(L0_attn_output + L0_input)\n",
    "L0_logits = keras.layers.Dense(1)(L0_embeds)\n",
    "L0_logits = tf.squeeze(L0_logits, axis=-1)\n",
    "\n",
    "L1_attn_output = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(L1_input, prompt_embeds)\n",
    "L1_embeds = keras.layers.LayerNormalization()(L1_attn_output + L1_input)\n",
    "L1_logits = keras.layers.Dense(1)(L1_embeds)\n",
    "L1_logits = tf.squeeze(L1_logits, axis=-1)\n",
    "\n",
    "L2_attn_output = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(L2_input, prompt_embeds)\n",
    "L2_embeds = keras.layers.LayerNormalization()(L2_attn_output + L2_input)\n",
    "L2_logits = keras.layers.Dense(1)(L2_embeds)\n",
    "L2_logits = tf.squeeze(L2_logits, axis=-1)\n",
    "\n",
    "L3_attn_output = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(L3_input, prompt_embeds)\n",
    "L3_embeds = keras.layers.LayerNormalization()(L3_attn_output + L3_input)\n",
    "L3_logits = keras.layers.Dense(1)(L3_embeds)\n",
    "L3_logits = tf.squeeze(L3_logits, axis=-1)\n",
    "\n",
    "L4_attn_output = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(L4_input, prompt_embeds)\n",
    "L4_embeds = keras.layers.LayerNormalization()(L4_attn_output + L4_input)\n",
    "L4_logits = keras.layers.Dense(1)(L4_embeds)\n",
    "L4_logits = tf.squeeze(L4_logits, axis=-1)\n",
    "\n",
    "\n",
    "L0_logits = keras.layers.Softmax(name='L0_preds')(L0_logits)\n",
    "L1_logits = keras.layers.Softmax(name='L1_preds')(L1_logits)\n",
    "L2_logits = keras.layers.Softmax(name='L2_preds')(L2_logits)\n",
    "L3_logits = keras.layers.Softmax(name='L3_preds')(L3_logits)\n",
    "L4_logits = keras.layers.Softmax(name='L4_preds')(L4_logits)\n",
    "\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[input_ids, attention_mask, L0_input, L1_input, L2_input, L3_input, L4_input],\n",
    "    outputs=[L0_logits, L1_logits, L2_logits, L3_logits, L4_logits]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(gt, preds):\n",
    "    losses = tf.convert_to_tensor([keras.losses.categorical_crossentropy(\n",
    "        gt_item, preds_item\n",
    "    ) for (gt_item, preds_item) in zip(gt, preds)])\n",
    "\n",
    "    loss = tf.reduce_sum(losses, axis=1)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.AdamW()\n",
    "model.compile(\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embeds = list(stack_embeds(BATCH_SIZE).values())\n",
    "\n",
    "# Metrics\n",
    "train_loss_metric = metrics.Mean(name='train_loss')\n",
    "train_accuracy_metrics = [\n",
    "    metrics.CategoricalAccuracy(name=f'train_accuracy_task_{i}') for i in range(5)\n",
    "]\n",
    "\n",
    "test_loss_metric = metrics.Mean(name='test_loss')\n",
    "test_accuracy_metrics = [\n",
    "    metrics.CategoricalAccuracy(name=f'test_accuracy_task_{i}') for i in range(5)\n",
    "]\n",
    "\n",
    "# Checkpointing setup\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, './checkpoints', max_to_keep=5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_ids, attention_mask, l0, l1, l2, l3, l4):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([input_ids, attention_mask] + label_embeds, training=True)\n",
    "        loss = loss_fn(\n",
    "            [l0, l1, l2, l3, l4],\n",
    "            predictions\n",
    "        )\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss_metric.update_state(loss)\n",
    "    for metric, labels, pred in zip(train_accuracy_metrics, [l0, l1, l2, l3, l4], predictions):\n",
    "        metric.update_state(labels, pred)\n",
    "\n",
    "# Testing step\n",
    "@tf.function\n",
    "def test_step(input_ids, attention_mask, l0, l1, l2, l3, l4):\n",
    "    predictions = model([input_ids, attention_mask] + label_embeds, training=False)\n",
    "    loss = loss_fn(\n",
    "        [l0, l1, l2, l3, l4],\n",
    "        predictions\n",
    "    )\n",
    "    \n",
    "    test_loss_metric.update_state(loss)\n",
    "    for metric, labels, pred in zip(test_accuracy_metrics, [l0, l1, l2, l3, l4], predictions):\n",
    "        metric.update_state(labels, pred)\n",
    "\n",
    "# Custom training loop\n",
    "def train_model(epochs, train_dataset, test_dataset):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        \n",
    "        # Training\n",
    "        progbar = Progbar(target=len(train_dataset), unit_name='step')\n",
    "        for step, (input_ids, attention_mask, l0, l1, l2, l3, l4) in enumerate(train_dataset):\n",
    "            train_step(input_ids, attention_mask, l0, l1, l2, l3, l4)\n",
    "            progbar.update(step + 1, values=[('loss', train_loss_metric.result())] + \n",
    "                            [(metric.name, metric.result()) for metric in train_accuracy_metrics])\n",
    "        \n",
    "        # Validation\n",
    "        print(\"\\nValidation:\")\n",
    "        progbar = Progbar(target=len(test_dataset), unit_name='step')\n",
    "        for step, (input_ids, attention_mask, l0, l1, l2, l3, l4) in enumerate(test_dataset):\n",
    "            test_step(input_ids, attention_mask, l0, l1, l2, l3, l4)\n",
    "            progbar.update(step + 1, values=[('loss', test_loss_metric.result())] + \n",
    "                            [(metric.name, metric.result()) for metric in test_accuracy_metrics])\n",
    "        \n",
    "        print(f'\\nEpoch {epoch + 1} Summary:')\n",
    "        print(f'Train Loss: {train_loss_metric.result():.4f}')\n",
    "        for metric in train_accuracy_metrics:\n",
    "            print(f'{metric.name}: {metric.result():.4f}')\n",
    "        \n",
    "        print(f'Test Loss: {test_loss_metric.result():.4f}')\n",
    "        for metric in test_accuracy_metrics:\n",
    "            print(f'{metric.name}: {metric.result():.4f}\\n')\n",
    "        \n",
    "        # Checkpointing\n",
    "        if test_loss_metric.result() < best_test_loss:\n",
    "            best_test_loss = test_loss_metric.result()\n",
    "            checkpoint_save_path = checkpoint_manager.save()\n",
    "            print(f\"Best model saved to {checkpoint_save_path} with loss {best_test_loss:.4f}\\n\")\n",
    "        \n",
    "        # Reset metrics at the end of each epoch\n",
    "        train_loss_metric.reset_states()\n",
    "        for metric in train_accuracy_metrics:\n",
    "            metric.reset_states()\n",
    "        test_loss_metric.reset_states()\n",
    "        for metric in test_accuracy_metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "train_model(10, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./v5-approach3/model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
