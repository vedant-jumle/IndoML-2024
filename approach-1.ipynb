{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 13:28:31.438649: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-11 13:28:31.438672: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-11 13:28:31.439454: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-11 13:28:31.443457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 13:28:32.060288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from pqdm.threads import pqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 13:28:33.756715: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:33.758040: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:33.779515: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:33.780822: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:33.780927: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:33.782205: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.103941: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.105294: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.105393: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.106686: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.106773: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.108039: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.116781: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.118063: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.118163: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.119408: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.119505: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.120904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43585 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-08-11 13:28:34.121236: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 13:28:34.121315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10376 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:03:00.0, compute capability: 8.6\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at models/v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "# model = TFT5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"models/v1\")\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(\"models/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path) as input_file:\n",
    "        lines = list(map(json.loads, input_file))\n",
    "\n",
    "    return lines\n",
    "\n",
    "def load_data(root_path):\n",
    "    files = os.listdir(root_path)\n",
    "    dataset= {}\n",
    "\n",
    "    for filename in files:\n",
    "        filepath = root_path + f'/{filename}'\n",
    "\n",
    "        dataset[filename] = load_jsonl(filepath)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data('data/L1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['llama3.1', 'gemma2:2b', 'gemma2']\n",
    "\n",
    "test_from = 'attrebute_val'\n",
    "train_from = 'attrebute_train'\n",
    "target_from = 'attrebute_test'\n",
    "\n",
    "train_inputs = train_from + '.data'\n",
    "train_labels = train_from + '.solution'\n",
    "\n",
    "test_inputs = test_from + '.data'\n",
    "test_labels = test_from + '.solution'\n",
    "\n",
    "target_inputs = target_from + '.data'\n",
    "\n",
    "test_inputs = dataset[test_inputs]\n",
    "test_labels = dataset[test_labels]\n",
    "train_inputs = dataset[train_inputs]\n",
    "train_labels = dataset[train_labels]\n",
    "target_inputs = dataset[target_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN=250\n",
    "\n",
    "def make_prompt(data):\n",
    "    if 'title' in data:\n",
    "        input_type = 'input'\n",
    "    else:\n",
    "        input_type = 'label'\n",
    "\n",
    "    prompt = {\n",
    "        \"label\": \"The product belongs to these categories:\\nBrand: {details_Brand}\\nLevel 0: {L0_category}\\nLevel 1: {L1_category}\\nLevel 2: {L2_category}\\nLevel 3: {L3_category}\\nLevel 4: {L4_category}\",\n",
    "        \"input\": \"Categories the product into 5 levels of categories and identify the brand:\\nTitle: {title}\\nStore:{store}\\nManufacturer:{details_Manufacturer}\"\n",
    "    }\n",
    "\n",
    "    return prompt[input_type].format(**data)\n",
    "\n",
    "def batch_make_prompt(data):\n",
    "    return [make_prompt(item) for item in data]\n",
    "# def tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba598f8b6824a92b624418defce04ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c8586126594a96a73d6ae22f45232a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b86d1677d54feb9655e536a320938c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7dcfc73b5842929af6dc1c89134112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4b80562cc64d38b801dade16e18959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7de2a11b7a3409ea22e0189eb61e6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/443499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7071c450db0d489495b7b7493c407898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6a2a23197541f193081456c55fc95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6e42dd4399400a9d6005834eeeb006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87737752852e4af0bcf2d724dbc7d14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66b069dfbd348a183c0dfcdf2f800d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c158bf393ffe45ccaf694ad1938103d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/95035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_input_prompts = pqdm(train_inputs, make_prompt, n_jobs=5)\n",
    "train_label_prompts = pqdm(train_labels, make_prompt, n_jobs=5)\n",
    "test_input_prompts = pqdm(test_inputs, make_prompt, n_jobs=5)\n",
    "test_label_prompts = pqdm(test_labels, make_prompt, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(inputs, labels):\n",
    "    inputs = [item.decode() for item in inputs]\n",
    "    labels = [item.decode() for item in labels]\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        inputs,\n",
    "        padding=\"max_length\",\n",
    "        max_length=SEQ_LEN,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "\n",
    "    input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "    target_encoding = tokenizer(\n",
    "        labels,\n",
    "        padding=\"max_length\",\n",
    "        max_length=SEQ_LEN,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "\n",
    "    labels = target_encoding.input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "\n",
    "def predict(inputs):\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        inputs,\n",
    "        padding=\"max_length\",\n",
    "        max_length=SEQ_LEN,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "\n",
    "    input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "    outputs = model.generate(input_ids, max_new_tokens=SEQ_LEN)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_input_prompts, train_label_prompts)).shuffle(1000).batch(BATCH_SIZE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_input_prompts, test_label_prompts)).shuffle(1000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.AdamW(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=5\n",
    "\n",
    "per_epoch_loss=[]\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    dataset_iter = train_ds.as_numpy_iterator()\n",
    "\n",
    "    progbar = Progbar(len(train_ds), stateful_metrics=['train_loss'])\n",
    "    loss_metric = Mean(\"train_loss\")\n",
    "\n",
    "    for i, batch in enumerate(dataset_iter):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = train_model(inputs, labels)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        loss_metric.update_state(loss)\n",
    "        values = [('train_loss', loss_metric.result())]\n",
    "\n",
    "        progbar.update(i+1, values=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models/v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iter = test_ds.as_numpy_iterator()\n",
    "\n",
    "progbar = Progbar(len(test_ds), stateful_metrics=['val_loss'])\n",
    "loss_metric = Mean(\"val_loss\")\n",
    "\n",
    "for i, batch in enumerate(dataset_iter):\n",
    "    inputs, labels = batch\n",
    "\n",
    "    loss = train_model(inputs, labels)\n",
    "\n",
    "    loss_metric.update_state(loss)\n",
    "    values = [('val_loss', loss_metric.result())]\n",
    "\n",
    "    progbar.update(i+1, values=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ds), len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365f77102045432e8e29f72f67a3cdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/95036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90482816e2474a8ebb5d5f0e51f4c3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/95036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7406fd3bdf69436699511946c5c36e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/95036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_prompts = pqdm(target_inputs, make_prompt, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_data(item):\n",
    "    item = item.numpy().decode('utf-8')\n",
    "    encoding = tokenizer.encode(\n",
    "        item,\n",
    "        padding=\"max_length\",\n",
    "        max_length=SEQ_LEN,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    return encoding[0]\n",
    "\n",
    "def batch_tokenize(batch):\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        batch,\n",
    "        padding=\"max_length\",\n",
    "        max_length=SEQ_LEN,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    return encoding.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ds = (tf.data.Dataset.from_tensor_slices(target_prompts)\n",
    "             .map(lambda a: tf.py_function(tokenizer_data, [a], [tf.int8]))\n",
    "             .batch(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(lst, n):\n",
    "    # Split the list into chunks of n items\n",
    "    chunks = [lst[i:i + n] for i in range(0, len(lst), n)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e18bb9aed94650906be98c67cec4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc69b308b2b491eaacfb8f098c837c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643dfb03b93f40d98b98f895a4c598ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batches = chunk_list(target_prompts, n=1024)\n",
    "batches = pqdm(batches, batch_tokenize, n_jobs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [26:51<00:00, 17.32s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "for input_ids in tqdm(batches):\n",
    "    outputs = model.generate(input_ids, max_new_tokens=SEQ_LEN)\n",
    "\n",
    "    answers.extend(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Brand:\\s?.+)? (Level 0:\\s?.+)? (Level 1:\\s?.+)? (Level 2:\\s?.+)? (Level 3:\\s?.+)? (Level 4:\\s?.+)?\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "word=r'.'\n",
    "pattern = rf\"(Brand:\\s?{word}+)? (Level 0:\\s?{word}+)? (Level 1:\\s?{word}+)? (Level 2:\\s?{word}+)? (Level 3:\\s?{word}+)? (Level 4:\\s?{word}+)?\"\n",
    "\n",
    "def extract_categories(output):\n",
    "\n",
    "    brand, l0, l1, l2, l3, l4 = re.findall(pattern, output)[0]\n",
    "    return {\n",
    "        'details_Brand': brand,\n",
    "        'L0_category': l0,\n",
    "        'L1_category': l1,\n",
    "        'L2_category': l2,\n",
    "        'L3_category': l3,\n",
    "        'L4_category': l4\n",
    "    }\n",
    "print(pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'details_Brand': 'LUXPaper', 'L0_category': 'Office Products', 'L1_category': 'Office & School Supplies', 'L2_category': 'Envelopes, Mailers & Shipping Supplies', 'L3_category': 'Envelopes', 'L4_category': 'Greeting Card Envelopes'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_categories(text):\n",
    "    key_map = {\n",
    "        'Brand': 'details_Brand',\n",
    "        'Level 0': 'L0_category',\n",
    "        'Level 1': 'L1_category',\n",
    "        'Level 2': 'L2_category',\n",
    "        'Level 3': 'L3_category',\n",
    "        'Level 4': 'L4_category'\n",
    "    }\n",
    "    \n",
    "    # Regex patterns to match each key\n",
    "    patterns = {\n",
    "        'Brand': r'Brand:\\s*(.+?)(?=\\s+Level|\\s*<\\/s>)',\n",
    "        'Level 0': r'Level 0:\\s*(.+?)(?=\\s+Level|\\s*<\\/s>)',\n",
    "        'Level 1': r'Level 1:\\s*(.+?)(?=\\s+Level|\\s*<\\/s>)',\n",
    "        'Level 2': r'Level 2:\\s*(.+?)(?=\\s+Level|\\s*<\\/s>)',\n",
    "        'Level 3': r'Level 3:\\s*(.+?)(?=\\s+Level|\\s*<\\/s>)',\n",
    "        'Level 4': r'Level 4:\\s*(.+?)(?=\\s+Level|\\s*<\\/s>)'\n",
    "    }\n",
    "    \n",
    "    # Extracting data using the patterns\n",
    "    extracted_data = {v: 'na' for v in key_map.values()}\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            extracted_data[key_map[key]] = match.group(1).strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "The product belongs to these categories: Brand: LUXPaper Level 0: Office Products Level 1: Office & School Supplies Level 2: Envelopes, Mailers & Shipping Supplies Level 3: Envelopes Level 4: Greeting Card Envelopes</s><pad><pad><pad><pad><pad><pad><pad><pad>\n",
    "\"\"\"\n",
    "\n",
    "result = extract_categories(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95036/95036 [00:01<00:00, 89530.47it/s]\n"
     ]
    }
   ],
   "source": [
    "final_outputs = []\n",
    "for i, item in enumerate(tqdm(answers)):\n",
    "    details = {'indoml_id': i}\n",
    "    details.update(extract_categories(item))\n",
    "\n",
    "    final_outputs.append(json.dumps(details) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './data/L1/output/'\n",
    "\n",
    "with open(output_path + 'attribute_test_topjourney_1.predict', 'w') as output_file:\n",
    "    output_file.writelines(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4156982/1962397062.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "item = answers[0]\n",
    "matches = re.findall(pattern, item)[0]\n",
    "key_map = {\n",
    "    'Brand':'details_Brand',\n",
    "    'Level 0':'L0_category',\n",
    "    'Level 1':'L1_category',\n",
    "    'Level 2':'L2_category',\n",
    "    'Level 3':'L3_category',\n",
    "    'Level 4':'L4_category' \n",
    "}\n",
    "\n",
    "details = dict.fromkeys(key_map.values(), '')\n",
    "for match in matches:\n",
    "    key, value = list(map(str.strip, item.split(':')))\n",
    "    key = key_map[key]\n",
    "\n",
    "    details[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> The product belongs to these categories: Brand: CURT Level 0: Automotive Level 1: Exterior Accessories Level 2: Towing Products & Winches Level 3: Hitch Accessories Level 4: Wiring</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Brand: CURT',\n",
       " 'Level 0: Automotive',\n",
       " 'Level 1: Exterior Accessories',\n",
       " 'Level 2: Towing Products & Winches',\n",
       " 'Level 3: Hitch Accessories Level 4:',\n",
       " '')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brand', 'Breeze']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, value = list(map(str.strip, matches[0].split(':')))\n",
    "key = key_map[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4156982/3710189397.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4156982/3830513535.py\u001b[0m in \u001b[0;36mextract_categories\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbrand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     return {\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m'details_Brand'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbrand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 6)"
     ]
    }
   ],
   "source": [
    "extract_categories(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
